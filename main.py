# main.py
import gradio as gr
import time
import os
from huggingface_hub import InferenceClient

from models.clients import get_models
from retrieval.searcher import retrieve_from_qdrant
from realtime.handler import realtime_search_on_word_count, reset_realtime, used_chunk_ids
from prompt.builder import build_context_and_prompt
from config import *
from utils.logger import logger, timing_logger

# Load models má»™t láº§n duy nháº¥t
embedder, qdrant_client, hf_client = get_models()

# Anti-spam submit liÃªn tá»¥c
_last_submit_time = 0.0


def process_question(user_question: str, history):
    global _last_submit_time

    # Chá»‘ng spam submit
    current_time = time.time()
    if current_time - _last_submit_time < 1.0:
        yield history
        return
    _last_submit_time = current_time

    logger.info(f"Nháº­n cÃ¢u há»i: {user_question}")
    t_total_start = time.time()

    # === 1. Retrieval khi áº¥n Gá»­i ===
    submit_chunks = retrieve_from_qdrant(user_question, TOP_K_VECTOR * 2, used_chunk_ids)
    for c in submit_chunks:
        c["source"] = "submit"
        c["final_score"] = c.get("final_score", 0) + SUBMIT_RETRIEVAL_BOOST

    # === 2. Merge vá»›i realtime chunks ===
    from realtime.handler import realtime_shown_chunks

    combined_chunks = list(submit_chunks)
    seen_ids = used_chunk_ids.copy()
    seen_ids.update(c["metadata"].get("chunk_id") for c in submit_chunks if c["metadata"].get("chunk_id"))

    for c in realtime_shown_chunks:
        cid = c["metadata"].get("chunk_id")
        if cid and cid not in seen_ids:
            c_copy = c.copy()
            c_copy["source"] = "realtime"
            combined_chunks.append(c_copy)
            seen_ids.add(cid)

    # Sáº¯p xáº¿p vÃ  láº¥y top
    combined_chunks.sort(key=lambda x: x.get("final_score", 0), reverse=True)
    final_chunks = combined_chunks[:TOP_K_FINAL]

    # === 3. KhÃ´ng tÃ¬m tháº¥y gÃ¬ ===
    if not final_chunks:
        response = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong cÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t hiá»‡n táº¡i."
        history.append({"role": "user", "content": user_question})
        history.append({"role": "assistant", "content": response})
        reset_realtime()
        yield history
        return

    # === 4. Táº¡o context + messages ===
    context, messages = build_context_and_prompt(final_chunks, user_question)

    # ThÃªm vÃ o lá»‹ch sá»­ chat
    history.append({"role": "user", "content": user_question})
    history.append({"role": "assistant", "content": ""})
    yield history

    # === 5. Streaming LLM - ÄÃƒ FIX HOÃ€N TOÃ€N ===
    response = ""
    first_token_time = None

    try:
        stream = hf_client.chat_completion(
            messages,
            max_tokens=1024,
            temperature=0.7,
            top_p=0.95,
            stream=True,
        )

        for message in stream:
            # Báº£o vá»‡ tuyá»‡t Ä‘á»‘i: náº¿u khÃ´ng cÃ³ choices â†’ bá» qua
            if not message.choices:
                continue

            delta = message.choices[0].delta

            # Má»™t sá»‘ chunk chá»‰ cÃ³ tool_calls hoáº·c finish_reason
            if delta.content is not None:
                token = delta.content
                response += token
                history[-1]["content"] = response

                # Ghi log thá»i gian ra token Ä‘áº§u tiÃªn
                if first_token_time is None:
                    first_token_time = time.time()
                    timing_logger.info(f"First token: {first_token_time - t_total_start:.2f}s")

                yield history

            # Dá»«ng sá»›m náº¿u model bÃ¡o xong
            if getattr(delta, "finish_reason", None) is not None:
                break

    except Exception as e:
        logger.error(f"Lá»—i khi gá»i mÃ´ hÃ¬nh: {e}")
        error_msg = "ÄÃ£ xáº£y ra lá»—i khi káº¿t ná»‘i Ä‘áº¿n mÃ´ hÃ¬nh ngÃ´n ngá»¯. Vui lÃ²ng thá»­ láº¡i sau Ã­t phÃºt."
        history[-1]["content"] = error_msg
        yield history
        return

    finally:
        # Cáº­p nháº­t chunk Ä‘Ã£ dÃ¹ng + reset realtime
        for c in final_chunks:
            if cid := c["metadata"].get("chunk_id"):
                used_chunk_ids.add(cid)
        reset_realtime()

        total_time = time.time() - t_total_start
        timing_logger.info(f"Tá»•ng thá»i gian xá»­ lÃ½: {total_time:.2f}s")

    # Káº¿t thÃºc hoÃ n toÃ n
    yield history


# ===================== GRADIO UI =====================
with gr.Blocks(title="Luáº­t sÆ° AI Viá»‡t Nam", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # âš–ï¸ Trá»£ lÃ½ PhÃ¡p lÃ½ Viá»‡t Nam  
    Há»i báº¥t ká»³ quy Ä‘á»‹nh phÃ¡p luáº­t nÃ o â€“ tÃ´i tráº£ lá»i chÃ­nh xÃ¡c theo vÄƒn báº£n hiá»‡n hÃ nh.
    """)

    chatbot = gr.Chatbot(height=620, type="messages", avatar_images=("ðŸ‘¤", "âš–ï¸"))

    with gr.Row():
        txt = gr.Textbox(
            label="CÃ¢u há»i cá»§a báº¡n",
            placeholder="VÃ­ dá»¥: Thuáº¿ TNCN vá»›i chuyÃªn gia nÆ°á»›c ngoÃ i theo nghá»‹ Ä‘á»‹nh nÃ o nÄƒm 2024?",
            lines=3,
            scale=8,
            container=False
        )
        send_btn = gr.Button("ðŸš€ Gá»­i", variant="primary", scale=1)

    # Realtime search khi Ä‘ang gÃµ
    txt.change(
        fn=realtime_search_on_word_count,
        inputs=txt,
        outputs=None
    )

    # Gá»­i cÃ¢u há»i
    send_btn.click(
        fn=process_question,
        inputs=[txt, chatbot],
        outputs=chatbot
    ).then(
        lambda: "",  # XÃ³a Ã´ input sau khi gá»­i
        outputs=txt
    )

    # Enter Ä‘á»ƒ gá»­i
    txt.submit(
        fn=process_question,
        inputs=[txt, chatbot],
        outputs=chatbot
    ).then(
        lambda: "",
        outputs=txt
    )

if __name__ == "__main__":
    print("Khá»Ÿi Ä‘á»™ng Trá»£ lÃ½ PhÃ¡p lÃ½ Viá»‡t Nam...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        favicon_path="favicon.ico" if os.path.exists("favicon.ico") else None
    )