<!doctype html>
<html lang="vi">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Simple RAG Voicebot UI - Fixed</title>
    <style>
      body{font-family: Arial, Helvetica, sans-serif; padding:20px}
      textarea{width:100%;height:120px}
      button{padding:10px 16px;margin-top:8px}
      #log{background:#111;color:#bfbfbf;padding:10px;height:200px;overflow:auto;font-size:12px}
      
      /* Responsive layout */
      .main-container {
        display: flex;
        gap: 20px;
        flex-wrap: wrap;
      }
      
      .voice-section {
        flex: 1;
        min-width: 300px;
      }
      
      .file-upload-section {
        flex: 0 0 280px;
        min-width: 280px;
      }
      
      /* Mobile: stack vertically */
      @media (max-width: 768px) {
        .main-container {
          flex-direction: column;
        }
        .file-upload-section {
          flex: 1;
        }
      }
    </style>
  </head>
  <body>
    <style>
      /* ·∫®n c√°c ph·∫ßn nh·∫≠p/g√µ text, log, player */
      #question, #askBtn, #respText, #player, #log { display: none !important; }
    </style>

    <div class="main-container">
      <!-- Voice section (left/main) -->
      <div class="voice-section">
        <h3>Voice Input</h3>
        <div>
            <button id="recordBtn" style="padding: 12px 24px; font-size: 16px; background: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;">
                üé§ Start Recording
            </button>
            <button id="stopBtn" style="padding: 12px 24px; font-size: 16px; background: #f44336; color: white; border: none; border-radius: 4px; display: none; cursor: pointer;">
                ‚èπÔ∏è Stop Recording
            </button>
            <span id="recordingStatus" style="margin-left: 10px; color: #666;"></span>
        </div>

        <div id="audioVisualizer" style="margin: 10px 0; height: 60px; border: 1px solid #ddd;"></div>

        <audio id="recordedAudio" controls style="width: 100%; margin: 10px 0;"></audio>
        
        <!-- Th√™m th√¥ng tin v·ªÅ sample rate -->
        <div id="sampleRateInfo" style="font-size: 12px; color: #666; margin-top: 5px;">
          Sample rate: <span id="currentSampleRate">-</span> Hz
        </div>
      </div>

      <!-- File upload section (right/sidebar) -->
      <div class="file-upload-section">
        <h4 style="margin-top: 0;">üìÅ Upload WAV</h4>
        <input type="file" id="wavInput" accept="audio/wav,audio/x-wav" style="display: none;">
        <div id="wavDropZone" style="padding: 30px 15px; border: 2px dashed #4CAF50; border-radius: 6px; background: #f6fff6; color: #333; cursor: pointer; text-align: center; font-size: 14px;">
          K√©o th·∫£ WAV v√†o ƒë√¢y<br>ho·∫∑c click ƒë·ªÉ ch·ªçn
        </div>
        <div id="wavStatus" style="margin-top: 8px; color: #666; font-size: 13px; text-align: center;"></div>
      </div>
    </div>

<script>
    // Voice recording functionality
    let mediaRecorder = null;
    let audioChunks = [];
    let isRecording = false;
    let audioContext = null;
    let analyser = null;
    let dataArray = null;
    let canvas = null;
    let canvasCtx = null;
    let actualSampleRate = 16000; // Will be set from AudioContext

    const recordBtn = document.getElementById('recordBtn');
    const stopBtn = document.getElementById('stopBtn');
    const recordingStatus = document.getElementById('recordingStatus');
    const audioVisualizer = document.getElementById('audioVisualizer');
    const recordedAudio = document.getElementById('recordedAudio');
    const currentSampleRate = document.getElementById('currentSampleRate');

    // Setup audio visualizer
    function setupVisualizer() {
        canvas = document.createElement('canvas');
        canvas.width = audioVisualizer.clientWidth;
        canvas.height = audioVisualizer.clientHeight;
        audioVisualizer.appendChild(canvas);
        canvasCtx = canvas.getContext('2d');
    }

    // Start recording
    async function startRecording() {
        try {
            // Immediately update UI to show recording state
            recordBtn.style.display = 'none';
            stopBtn.style.display = 'inline-block';
            recordingStatus.textContent = 'üé§ Recording...';
            recordingStatus.style.color = '#f44336';
            
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    sampleRate: 16000 // Y√™u c·∫ßu sample rate th·∫•p h∆°n ngay t·ª´ ƒë·∫ßu
                }
            });
            
            // Setup audio context for visualization
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            actualSampleRate = audioContext.sampleRate; // Capture actual sample rate (usually 48000)
            currentSampleRate.textContent = actualSampleRate;
            console.log('Detected sample rate:', actualSampleRate);
            
            analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);
            
            // Setup visualizer
            if (!canvas) setupVisualizer();
            
            // Start visualization
            drawVisualizer();
            
            // Setup media recorder v·ªõi c·∫•u h√¨nh ch·∫•t l∆∞·ª£ng th·∫•p h∆°n
            const options = {
                mimeType: 'audio/webm;codecs=opus',
                audioBitsPerSecond: 16000 // Bitrate th·∫•p h∆°n
            };
            
            try {
                mediaRecorder = new MediaRecorder(stream, options);
            } catch (e) {
                console.warn('WebM/Opus not supported, using default:', e);
                mediaRecorder = new MediaRecorder(stream);
            }
            
            audioChunks = [];
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunks.push(event.data);
                }
            };
            
            mediaRecorder.onstop = async () => {
                // Create audio blob
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const audioUrl = URL.createObjectURL(audioBlob);
                recordedAudio.src = audioUrl;
                
                // Chuy·ªÉn ƒë·ªïi sample rate v√† transcribe
                await resampleAndTranscribe(audioBlob, actualSampleRate, 16000);
                
                // Cleanup
                stream.getTracks().forEach(track => track.stop());
                if (audioContext) audioContext.close();
            };
            
            // Start recording
            mediaRecorder.start(1000); // Collect data every second
            isRecording = true;
            
        } catch (error) {
            console.error('Error starting recording:', error);
            alert('Cannot access microphone. Please check permissions.');
            // Reset UI on error
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚ùå Error accessing microphone';
            recordingStatus.style.color = '#f44336';
        }
    }

    // H√†m resample audio t·ª´ sample rate cao xu·ªëng 16000
    async function resampleAndTranscribe(audioBlob, originalSampleRate, targetSampleRate = 16000) {
        try {
            recordingStatus.textContent = '‚è≥ Resampling audio...';
            
            // T·∫°o AudioContext m·ªõi ƒë·ªÉ x·ª≠ l√Ω resample
            const offlineContext = new OfflineAudioContext(
                1, // Mono
                Math.ceil(audioBlob.size * targetSampleRate / originalSampleRate),
                targetSampleRate
            );
            
            // T·∫°o AudioBuffer t·ª´ blob
            const arrayBuffer = await audioBlob.arrayBuffer();
            const audioBuffer = await offlineContext.decodeAudioData(arrayBuffer);
            
            // T·∫°o source t·ª´ buffer
            const source = offlineContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(offlineContext.destination);
            source.start(0);
            
            // Render audio v·ªõi sample rate m·ªõi
            const resampledBuffer = await offlineContext.startRendering();
            
            // Chuy·ªÉn AudioBuffer th√†nh WAV bytes
            const wavBytes = audioBufferToWav(resampledBuffer, targetSampleRate);
            
            // T·∫°o blob m·ªõi v·ªõi sample rate ƒë√£ chuy·ªÉn ƒë·ªïi
            const resampledBlob = new Blob([wavBytes], { type: 'audio/wav' });
            
            console.log(`Resampled audio from ${originalSampleRate}Hz to ${targetSampleRate}Hz`);
            currentSampleRate.textContent = `${originalSampleRate} ‚Üí ${targetSampleRate} Hz`;
            
            // G·ª≠i audio ƒë√£ resample ƒë·ªÉ transcribe
            await transcribeAudio(resampledBlob, targetSampleRate);
            
        } catch (error) {
            console.error('Error resampling audio:', error);
            // Fallback: g·ª≠i audio g·ªëc n·∫øu resample th·∫•t b·∫°i
            recordingStatus.textContent = '‚è≥ Transcribing (original sample rate)...';
            await transcribeAudio(audioBlob, originalSampleRate);
        }
    }

    // H√†m chuy·ªÉn AudioBuffer th√†nh WAV bytes
    function audioBufferToWav(buffer, sampleRate) {
        const numChannels = buffer.numberOfChannels;
        const length = buffer.length * numChannels * 2;
        const data = new DataView(new ArrayBuffer(44 + length));
        
        // Write WAV header
        writeString(data, 0, 'RIFF');
        data.setUint32(4, 36 + length, true);
        writeString(data, 8, 'WAVE');
        writeString(data, 12, 'fmt ');
        data.setUint32(16, 16, true);
        data.setUint16(20, 1, true); // PCM format
        data.setUint16(22, numChannels, true);
        data.setUint32(24, sampleRate, true);
        data.setUint32(28, sampleRate * numChannels * 2, true);
        data.setUint16(32, numChannels * 2, true);
        data.setUint16(34, 16, true); // 16-bit
        writeString(data, 36, 'data');
        data.setUint32(40, length, true);
        
        // Write PCM data
        let offset = 44;
        for (let i = 0; i < buffer.length; i++) {
            for (let channel = 0; channel < numChannels; channel++) {
                const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
                data.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                offset += 2;
            }
        }
        
        return new Uint8Array(data.buffer);
    }

    function writeString(data, offset, string) {
        for (let i = 0; i < string.length; i++) {
            data.setUint8(offset + i, string.charCodeAt(i));
        }
    }

    // Stop recording
    function stopRecording() {
        if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚úÖ Recording complete';
            recordingStatus.style.color = '#4CAF50';
        }
    }

    // Draw audio visualizer
    function drawVisualizer() {
        if (!isRecording || !analyser || !canvasCtx) return;
        
        requestAnimationFrame(drawVisualizer);
        
        analyser.getByteFrequencyData(dataArray);
        
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = (canvas.width / dataArray.length) * 2.5;
        let barHeight;
        let x = 0;
        
        for (let i = 0; i < dataArray.length; i++) {
            barHeight = dataArray[i] / 2;
            
            canvasCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
            canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
            
            x += barWidth + 1;
        }
    }
    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
      });
    }


    // X·ª≠ l√Ω file wav ƒë∆∞·ª£c ch·ªçn ho·∫∑c th·∫£ v√†o
    async function handleWavFile(file) {
        if (!file || !file.type.match(/^audio\/(wav|x-wav)$/)) {
            wavStatus.textContent = '‚ùå File kh√¥ng h·ª£p l·ªá';
            return;
        }
        wavStatus.textContent = '‚è≥ ƒêang x·ª≠ l√Ω...';
        try {
            // Hi·ªÉn th·ªã file l√™n audio player
            const url = URL.createObjectURL(file);
            recordedAudio.src = url;
            // G·ª≠i file l√™n /transcribe ƒë·ªÉ l·∫•y text
            const formData = new FormData();
            formData.append('file', file);
            formData.append('stream', 'false');
            const resp = await fetch('/transcribe', { method: 'POST', body: formData });
            const result = await resp.json();
            if (result.success && result.text) {
                wavStatus.textContent = `‚úÖ "${result.text.substring(0, 30)}..."`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                let msg = '‚ùå Nh·∫≠n di·ªán th·∫•t b·∫°i';
                if (!result.text || (typeof result.text === 'string' && result.text.trim() === '')) {
                    msg = '‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i';
                }
                wavStatus.textContent = msg;
            }
        } catch (e) {
            wavStatus.textContent = '‚ùå L·ªói khi g·ª≠i file';
            console.error(e);
        }
    }


    // Transcribe audio to text (ƒë√£ resample)
    async function transcribeAudio(audioBlob, sampleRate = 16000) {
        try {
            recordingStatus.textContent = '‚è≥ Transcribing...';
            
            // Convert to base64
            const base64Audio = await blobToBase64(audioBlob);
            
            // Use new endpoint v·ªõi sample rate ƒë√£ resample
            const response = await fetch('/transcribe_base64', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    audio_data: base64Audio.split(',')[1],
                    sample_rate: sampleRate, // S·ª≠ d·ª•ng sample rate ƒë√£ resample
                    stream: false,
                    mime_type: audioBlob.type
                })
            });
            
            const result = await response.json();
            if (result.success && result.text) {
                recordingStatus.textContent = `‚úÖ Transcribed: ${result.text.substring(0, 50)}...`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                let msg = '‚ùå Transcription failed';
                if (!result.text || (typeof result.text === 'string' && result.text.trim() === '')) {
                    msg = 'Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i n√†o t·ª´ ng∆∞·ªùi d√πng, c√≥ l·∫Ω h·ªç ch∆∞a n√≥i g√¨, ho·∫∑c mic c√≥ v·∫•n ƒë·ªÅ';
                }
                recordingStatus.textContent = msg;
                console.error('Transcription error:', result.error);
            }
        } catch (error) {
            console.error('Transcription error:', error);
            recordingStatus.textContent = '‚ùå Transcription error';
        }
    }

    async function sendTextForVoiceResponse(text) {
      try {
          recordingStatus.textContent = 'üéµ Generating voice response...';
          
          const response = await fetch('/ask', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ question: text })
          });
          
          const result = await response.json();
          if (result.id && result.text) {
              recordingStatus.textContent = 'üéµ Voice response ready!';
              // B·∫°n c√≥ th·ªÉ t·ª± ƒë·ªông play audio response ·ªü ƒë√¢y
              console.log('Voice response ID:', result.id, 'Text:', result.text);
                  // B·∫Øt ƒë·∫ßu polling tr·∫°ng th√°i ƒë·ªÉ l·∫•y v√† ph√°t audio
                  pollAndPlayAudio(result.id);
          }
      } catch (error) {
          console.error('Voice response error:', error);
          recordingStatus.textContent = '‚ùå Voice response failed';
      }
    }


        // H√†m t·ª± ƒë·ªông polling tr·∫°ng th√°i v√† ph√°t audio khi c√≥
        let lastPlayedQuick = null;
        let lastPlayedFull = [];


        // Bi·∫øn global ƒë·ªÉ qu·∫£n l√Ω
        let audioQueue = {
            items: [],
            isPlaying: false,
            currentTaskId: null
        };

        // H√†m c·∫£i ti·∫øn pollAndPlayAudio
        async function pollAndPlayAudio(taskId) {
            audioQueue.currentTaskId = taskId;
            let taskCompleted = false;
            let quickAudioAdded = false;
            
            // Polling cho ƒë·∫øn khi task ho√†n th√†nh
            while (!taskCompleted) {
                try {
                    const resp = await fetch(`/status/${taskId}`);
                    const status = await resp.json();
                    
                    // Th√™m quick audio n·∫øu c√≥ (∆∞u ti√™n cao)
                    if (status.quick_audio_url && !quickAudioAdded) {
                        addToAudioQueue({
                            url: status.quick_audio_url,
                            type: 'quick',
                            priority: 10, // Highest priority
                            taskId: taskId
                        });
                        quickAudioAdded = true;
                    }
                    
                    // Th√™m full audio parts
                    if (Array.isArray(status.full_audio_urls)) {
                        status.full_audio_urls.forEach((url, index) => {
                            if (!isAudioInQueue(url)) {
                                addToAudioQueue({
                                    url: url,
                                    type: 'full',
                                    priority: 5 - index, // Earlier parts get higher priority
                                    taskId: taskId
                                });
                            }
                        });
                    }
                    
                    // Ki·ªÉm tra n·∫øu c√≥ merged audio (s·∫Ω thay th·∫ø c√°c parts)
                    if (status.full_merged_url) {
                        // Clear queue for this task and add merged audio
                        clearQueueForTask(taskId);
                        addToAudioQueue({
                            url: status.full_merged_url,
                            type: 'merged',
                            priority: 8, // Higher than full parts
                            taskId: taskId
                        });
                    }
                    
                    // T·ª± ƒë·ªông b·∫Øt ƒë·∫ßu ph√°t n·∫øu ch∆∞a ph√°t
                    if (!audioQueue.isPlaying && audioQueue.items.length > 0) {
                        playNextInQueue();
                    }
                    
                    // Ki·ªÉm tra n·∫øu task ƒë√£ ho√†n th√†nh
                    if (status.status === 'full_ready' || status.full_merged_url) {
                        // ƒê·ª£i queue c·ªßa task n√†y ph√°t xong
                        const hasUnplayed = audioQueue.items.some(item => 
                            item.taskId === taskId && !item.played
                        );
                        
                        if (!hasUnplayed) {
                            taskCompleted = true;
                            recordingStatus.textContent = '‚úÖ Finished!';
                            // Clean up old audio files
                            cleanupOldAudio();
                            break;
                        }
                    }
                    
                    // Hi·ªÉn th·ªã th√¥ng tin queue
                    updateQueueDisplay();
                    
                } catch (e) {
                    console.error('Polling error:', e);
                    // N·∫øu l·ªói nhi·ªÅu l·∫ßn, th·ª≠ l·∫°i sau
                    await new Promise(r => setTimeout(r, 2000));
                }
                
                // Poll interval
                await new Promise(r => setTimeout(r, 1000));
            }
        }

        function addToAudioQueue(item) {
            // Ki·ªÉm tra tr√πng l·∫∑p
            if (isAudioInQueue(item.url)) return;
            
            item.addedAt = Date.now();
            item.played = false;
            
            audioQueue.items.push(item);
            
            // S·∫Øp x·∫øp: priority cao tr∆∞·ªõc, r·ªìi ƒë·∫øn th·ªùi gian th√™m
            audioQueue.items.sort((a, b) => {
                if (b.priority !== a.priority) {
                    return b.priority - a.priority;
                }
                return a.addedAt - b.addedAt;
            });
            
            console.log(`Audio queued: ${item.url} (priority: ${item.priority})`);
        }

        function isAudioInQueue(url) {
            return audioQueue.items.some(item => item.url === url);
        }

        function clearQueueForTask(taskId) {
            audioQueue.items = audioQueue.items.filter(item => item.taskId !== taskId);
        }

        async function playNextInQueue() {
            if (audioQueue.isPlaying || audioQueue.items.length === 0) return;
            
            // T√¨m item ch∆∞a ph√°t ƒë·∫ßu ti√™n
            const nextItem = audioQueue.items.find(item => !item.played);
            if (!nextItem) return;
            
            audioQueue.isPlaying = true;
            nextItem.played = true;
            nextItem.startedAt = Date.now();
            
            try {
                recordingStatus.textContent = `üîä Playing ${nextItem.type} response...`;
                
                // Ph√°t audio
                await playAudioUrl(nextItem.url);
                
                nextItem.endedAt = Date.now();
                nextItem.duration = nextItem.endedAt - nextItem.startedAt;
                
                console.log(`Played: ${nextItem.url} (${Math.round(nextItem.duration/1000)}s)`);
                
            } catch (error) {
                console.error('Playback failed:', error);
                // Mark as played ƒë·ªÉ kh√¥ng b·ªã k·∫πt
                nextItem.endedAt = Date.now();
                nextItem.error = true;
            }
            
            audioQueue.isPlaying = false;
            
            // T·ª± ƒë·ªông ph√°t ti·∫øp n·∫øu c√≤n
            if (audioQueue.items.some(item => !item.played)) {
                setTimeout(playNextInQueue, 100); // Small delay
            } else {
                recordingStatus.textContent = 'Queue empty';
            }
            
            updateQueueDisplay();
        }

        function updateQueueDisplay() {
            const waiting = audioQueue.items.filter(item => !item.played).length;
            const playing = audioQueue.isPlaying ? 1 : 0;
            
            // C√≥ th·ªÉ hi·ªÉn th·ªã ·ªü ƒë√¢u ƒë√≥ tr√™n UI
            console.log(`Queue: ${playing} playing, ${waiting} waiting`);
            
            // Ho·∫∑c hi·ªÉn th·ªã trong m·ªôt div
            const queueInfo = document.getElementById('queueInfo');
            if (queueInfo) {
                queueInfo.innerHTML = `
                    <div>Playing: ${playing}</div>
                    <div>Waiting: ${waiting}</div>
                    <div>Total: ${audioQueue.items.length}</div>
                `;
            }
        }

        // Cleanup old audio files
        async function cleanupOldAudio() {
            try {
                // L·∫•y danh s√°ch c√°c file c≈© ƒë·ªÉ x√≥a
                const response = await fetch('/cleanup_old_audio', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ olderThan: 300 }) // 5 ph√∫t
                });
                const result = await response.json();
                console.log('Cleanup result:', result);
            } catch (e) {
                console.error('Cleanup failed:', e);
            }
        }



        // H√†m ph√°t audio t·ª´ url, tr·∫£ promise khi ph√°t xong
        function playAudioUrl(url) {
            return new Promise((resolve) => {
                recordedAudio.src = url;
                recordedAudio.play();
                recordedAudio.onended = () => {
                    resolve();
                };
                recordedAudio.onerror = () => {
                    resolve();
                };
            });
        }


    // Event listeners
    recordBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', stopRecording);

    // Event cho input file
    const wavInput = document.getElementById('wavInput');
    const wavDropZone = document.getElementById('wavDropZone');
    const wavStatus = document.getElementById('wavStatus');
    wavInput.addEventListener('change', (e) => {
        if (e.target.files && e.target.files[0]) {
            handleWavFile(e.target.files[0]);
        }
    });
    // Drag & drop
    wavDropZone.addEventListener('dragover', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#e0ffe0';
    });
    wavDropZone.addEventListener('dragleave', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
    });
    wavDropZone.addEventListener('drop', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
        if (e.dataTransfer.files && e.dataTransfer.files[0]) {
            handleWavFile(e.dataTransfer.files[0]);
        }
    });
    wavDropZone.addEventListener('click', () => {
        wavInput.click();
    });

    // Auto-stop recording after 30 seconds
    setInterval(() => {
        if (isRecording) {
            const recordingTime = audioChunks.length; // approx seconds
            if (recordingTime >= 30) {
                stopRecording();
            }
        }
    }, 1000);
</script>
  </body>
</html>