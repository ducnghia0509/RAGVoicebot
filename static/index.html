<!doctype html>
<html lang="vi">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Simple RAG Voicebot UI - Fixed</title>
    <style>
      body{font-family: Arial, Helvetica, sans-serif; padding:20px}
      textarea{width:100%;height:120px}
      button{padding:10px 16px;margin-top:8px}
      #log{background:#111;color:#bfbfbf;padding:10px;height:200px;overflow:auto;font-size:12px}
      
      /* Responsive layout */
      .main-container {
        display: flex;
        gap: 20px;
        flex-wrap: wrap;
      }
      
      .voice-section {
        flex: 1;
        min-width: 300px;
      }
      
      .file-upload-section {
        flex: 0 0 280px;
        min-width: 280px;
        display: none;
      }
      
      /* Mobile: stack vertically */
      @media (max-width: 768px) {
        .main-container {
          flex-direction: column;
        }
        .file-upload-section {
          flex: 1;
        }
      }
      
      /* Th√™m style cho queue info */
      .queue-info {
        background: #f0f8ff;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
        font-size: 12px;
        border-left: 3px solid #2196F3;
      }
    </style>
  </head>
  <body>
    <style>
      /* ·∫®n c√°c ph·∫ßn nh·∫≠p/g√µ text, log, player */
      #question, #askBtn, #respText, #player, #log { display: none !important; }
    </style>

    <div class="main-container">
      <!-- Voice section (left/main) -->
      <div class="voice-section">
        <h3>Voice Input</h3>
        <div>
            <button id="recordBtn" style="padding: 12px 24px; font-size: 16px; background: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;">
                üé§ Start Recording
            </button>
            <button id="stopBtn" style="padding: 12px 24px; font-size: 16px; background: #f44336; color: white; border: none; border-radius: 4px; display: none; cursor: pointer;">
                ‚èπÔ∏è Stop Recording
            </button>
            <span id="recordingStatus" style="margin-left: 10px; color: #666;"></span>
        </div>

        <div id="audioVisualizer" style="margin: 10px 0; height: 60px; border: 1px solid #ddd;"></div>

        <audio id="recordedAudio" controls style="width: 100%; margin: 10px 0;"></audio>
        
        <!-- Th√™m th√¥ng tin v·ªÅ sample rate -->
        <div id="sampleRateInfo" style="font-size: 12px; color: #666; margin-top: 5px;">
          Sample rate: <span id="currentSampleRate">-</span> Hz
        </div>
        
        <!-- Hi·ªÉn th·ªã queue info -->
        <div id="queueInfo" class="queue-info" style="display: none;">
          <strong>Audio Queue:</strong>
          <div id="queueStatus">Idle</div>
        </div>
      </div>

      <!-- File upload section (right/sidebar) -->
      <div class="file-upload-section">
        <h4 style="margin-top: 0;">üìÅ Upload WAV</h4>
        <input type="file" id="wavInput" accept="audio/wav,audio/x-wav" style="display: none;">
        <div id="wavDropZone" style="padding: 30px 15px; border: 2px dashed #4CAF50; border-radius: 6px; background: #f6fff6; color: #333; cursor: pointer; text-align: center; font-size: 14px;">
          K√©o th·∫£ WAV v√†o ƒë√¢y<br>ho·∫∑c click ƒë·ªÉ ch·ªçn
        </div>
        <div id="wavStatus" style="margin-top: 8px; color: #666; font-size: 13px; text-align: center;"></div>
      </div>
    </div>

<script>
    // Voice recording functionality
    let mediaRecorder = null;
    let audioChunks = [];
    let isRecording = false;
    let audioContext = null;
    let analyser = null;
    let dataArray = null;
    let canvas = null;
    let canvasCtx = null;
    let actualSampleRate = 16000; // Will be set from AudioContext

    const recordBtn = document.getElementById('recordBtn');
    const stopBtn = document.getElementById('stopBtn');
    const recordingStatus = document.getElementById('recordingStatus');
    const audioVisualizer = document.getElementById('audioVisualizer');
    const recordedAudio = document.getElementById('recordedAudio');
    const currentSampleRate = document.getElementById('currentSampleRate');
    const queueInfo = document.getElementById('queueInfo');
    const queueStatus = document.getElementById('queueStatus');

    // Setup audio visualizer
    function setupVisualizer() {
        canvas = document.createElement('canvas');
        canvas.width = audioVisualizer.clientWidth;
        canvas.height = audioVisualizer.clientHeight;
        audioVisualizer.appendChild(canvas);
        canvasCtx = canvas.getContext('2d');
    }

    // Start recording
    async function startRecording() {
        try {
            // Immediately update UI to show recording state
            recordBtn.style.display = 'none';
            stopBtn.style.display = 'inline-block';
            recordingStatus.textContent = 'üé§ Recording...';
            recordingStatus.style.color = '#f44336';
            
            const stream = await navigator.mediaDevices.getUserMedia({ 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
            
            // Setup audio context for visualization
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            actualSampleRate = audioContext.sampleRate; // Capture actual sample rate (usually 48000)
            currentSampleRate.textContent = actualSampleRate;
            console.log('Detected sample rate:', actualSampleRate);
            
            analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);
            
            // Setup visualizer
            if (!canvas) setupVisualizer();
            
            // Start visualization
            drawVisualizer();
            
            // Setup media recorder v·ªõi c·∫•u h√¨nh ch·∫•t l∆∞·ª£ng th·∫•p h∆°n
            const options = {
                mimeType: 'audio/webm;codecs=opus'
            };
            
            try {
                mediaRecorder = new MediaRecorder(stream, options);
            } catch (e) {
                console.warn('WebM/Opus not supported, using default:', e);
                mediaRecorder = new MediaRecorder(stream);
            }
            
            audioChunks = [];
            let recordingStartTime = Date.now();
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunks.push(event.data);
                }
            };
            
            mediaRecorder.onstop = async () => {
                // Create audio blob
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                const audioUrl = URL.createObjectURL(audioBlob);
                recordedAudio.src = audioUrl;
                
                // Chuy·ªÉn ƒë·ªïi sample rate v√† transcribe
                await resampleAndProcess(audioBlob, actualSampleRate, 16000);
                
                // Cleanup
                stream.getTracks().forEach(track => track.stop());
                if (audioContext) audioContext.close();
            };
            
            // Start recording
            mediaRecorder.start(1000); // Collect data every second
            isRecording = true;
            
        } catch (error) {
            console.error('Error starting recording:', error);
            alert('Cannot access microphone. Please check permissions.');
            // Reset UI on error
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚ùå Error accessing microphone';
            recordingStatus.style.color = '#f44336';
        }
    }

    // H√†m resample audio t·ª´ sample rate cao xu·ªëng 16000 v√† x·ª≠ l√Ω
    async function resampleAndProcess(audioBlob, originalSampleRate, targetSampleRate = 16000) {
        try {
            recordingStatus.textContent = '‚è≥ Resampling audio...';
            console.log(`Resampling from ${originalSampleRate}Hz to ${targetSampleRate}Hz`);
            
            // S·ª≠ d·ª•ng h√†m resample audio c·∫£i ti·∫øn
            const resampledBlob = await resampleAudio(audioBlob, originalSampleRate, targetSampleRate);
            
            console.log(`Successfully resampled to ${targetSampleRate}Hz`);
            currentSampleRate.textContent = `${originalSampleRate} ‚Üí ${targetSampleRate} Hz`;
            
            // G·ª≠i audio ƒë√£ resample ƒë·ªÉ transcribe
            await transcribeAudio(resampledBlob, targetSampleRate);
            
        } catch (error) {
            console.error('Error resampling audio:', error);
            // Fallback: g·ª≠i audio g·ªëc n·∫øu resample th·∫•t b·∫°i
            recordingStatus.textContent = '‚è≥ Transcribing (original sample rate)...';
            currentSampleRate.textContent = `${originalSampleRate} Hz (no resample)`;
            await transcribeAudio(audioBlob, originalSampleRate);
        }
    }

    // H√†m resample audio c·∫£i ti·∫øn
    async function resampleAudio(audioBlob, originalSampleRate, targetSampleRate) {
        try {
            // T·∫°o AudioContext t·∫°m ƒë·ªÉ decode audio
            const tempContext = new (window.AudioContext || window.webkitAudioContext)();
            
            // T·∫°o ArrayBuffer t·ª´ blob
            const arrayBuffer = await audioBlob.arrayBuffer();
            
            // Decode audio data
            const audioBuffer = await tempContext.decodeAudioData(arrayBuffer);
            
            // ƒê√≥ng context t·∫°m
            await tempContext.close();
            
            // T·∫°o OfflineAudioContext v·ªõi sample rate m·ª•c ti√™u
            const offlineContext = new OfflineAudioContext(
                audioBuffer.numberOfChannels,
                Math.ceil(audioBuffer.length * targetSampleRate / audioBuffer.sampleRate),
                targetSampleRate
            );
            
            // T·∫°o source v√† k·∫øt n·ªëi
            const source = offlineContext.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(offlineContext.destination);
            source.start(0);
            
            // Render audio v·ªõi sample rate m·ªõi
            const resampledBuffer = await offlineContext.startRendering();
            
            // Chuy·ªÉn th√†nh WAV bytes
            const wavBytes = audioBufferToWav(resampledBuffer, targetSampleRate);
            
            // T·∫°o blob WAV m·ªõi
            return new Blob([wavBytes], { type: 'audio/wav' });
            
        } catch (error) {
            console.error('Resample failed:', error);
            throw error;
        }
    }

    // H√†m chuy·ªÉn AudioBuffer th√†nh WAV bytes (phi√™n b·∫£n c·∫£i ti·∫øn)
    function audioBufferToWav(buffer, sampleRate) {
        const numChannels = buffer.numberOfChannels;
        const length = buffer.length * numChannels * 2;
        const data = new DataView(new ArrayBuffer(44 + length));
        
        // WAV header
        writeString(data, 0, 'RIFF');
        data.setUint32(4, 36 + length, true);
        writeString(data, 8, 'WAVE');
        writeString(data, 12, 'fmt ');
        data.setUint32(16, 16, true);
        data.setUint16(20, 1, true); // PCM format
        data.setUint16(22, numChannels, true);
        data.setUint32(24, sampleRate, true);
        data.setUint32(28, sampleRate * numChannels * 2, true);
        data.setUint16(32, numChannels * 2, true);
        data.setUint16(34, 16, true); // 16-bit
        writeString(data, 36, 'data');
        data.setUint32(40, length, true);
        
        // Write PCM data
        let offset = 44;
        
        // L·∫•y data t·ª´ t·∫•t c·∫£ channels
        const channelData = [];
        for (let channel = 0; channel < numChannels; channel++) {
            channelData.push(buffer.getChannelData(channel));
        }
        
        // Interleave channels
        for (let i = 0; i < buffer.length; i++) {
            for (let channel = 0; channel < numChannels; channel++) {
                const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
                data.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
                offset += 2;
            }
        }
        
        return new Uint8Array(data.buffer);
    }

    function writeString(data, offset, string) {
        for (let i = 0; i < string.length; i++) {
            data.setUint8(offset + i, string.charCodeAt(i));
        }
    }

    // Stop recording
    function stopRecording() {
        if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚úÖ Recording complete';
            recordingStatus.style.color = '#4CAF50';
        }
    }

    // Draw audio visualizer
    function drawVisualizer() {
        if (!isRecording || !analyser || !canvasCtx) return;
        
        requestAnimationFrame(drawVisualizer);
        
        analyser.getByteFrequencyData(dataArray);
        
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = (canvas.width / dataArray.length) * 2.5;
        let barHeight;
        let x = 0;
        
        for (let i = 0; i < dataArray.length; i++) {
            barHeight = dataArray[i] / 2;
            
            canvasCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
            canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
            
            x += barWidth + 1;
        }
    }

    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
      });
    }

    // X·ª≠ l√Ω file wav ƒë∆∞·ª£c ch·ªçn ho·∫∑c th·∫£ v√†o
    async function handleWavFile(file) {
        if (!file || !file.type.match(/^audio\/(wav|x-wav)$/)) {
            wavStatus.textContent = '‚ùå File kh√¥ng h·ª£p l·ªá';
            return;
        }
        wavStatus.textContent = '‚è≥ ƒêang x·ª≠ l√Ω...';
        try {
            // Hi·ªÉn th·ªã file l√™n audio player
            const url = URL.createObjectURL(file);
            recordedAudio.src = url;
            
            // G·ª≠i file l√™n /transcribe ƒë·ªÉ l·∫•y text
            const formData = new FormData();
            formData.append('file', file);
            formData.append('stream', 'false');
            
            const resp = await fetch('/transcribe', { 
                method: 'POST', 
                body: formData 
            });
            
            const result = await resp.json();
            if (result.success && result.text) {
                wavStatus.textContent = `‚úÖ "${result.text.substring(0, 30)}..."`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                let msg = '‚ùå Nh·∫≠n di·ªán th·∫•t b·∫°i';
                if (!result.text || (typeof result.text === 'string' && result.text.trim() === '')) {
                    msg = '‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i';
                }
                wavStatus.textContent = msg;
            }
        } catch (e) {
            wavStatus.textContent = '‚ùå L·ªói khi g·ª≠i file';
            console.error(e);
        }
    }

    // Transcribe audio to text (ƒë√£ resample)
    async function transcribeAudio(audioBlob, sampleRate = 16000) {
        try {
            recordingStatus.textContent = '‚è≥ Transcribing...';
            
            // Convert to base64
            const base64Audio = await blobToBase64(audioBlob);
            
            // Use endpoint /transcribe_base64 v·ªõi sample rate ƒë√£ resample
            const response = await fetch('/transcribe_base64', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    audio_data: base64Audio.split(',')[1],
                    sample_rate: sampleRate, // S·ª≠ d·ª•ng sample rate ƒë√£ resample
                    stream: false,
                    mime_type: audioBlob.type || 'audio/wav'
                })
            });
            
            const result = await response.json();
            
            if (result.success && result.text) {
                const transcribedText = result.text.trim();
                if (transcribedText) {
                    recordingStatus.textContent = `‚úÖ Transcribed: ${transcribedText.substring(0, 50)}...`;
                    // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                    await sendTextForVoiceResponse(transcribedText);
                } else {
                    recordingStatus.textContent = '‚ö†Ô∏è Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i (audio silent)';
                }
            } else {
                let msg = '‚ùå Transcription failed';
                if (result.error) {
                    msg += `: ${result.error}`;
                }
                recordingStatus.textContent = msg;
                console.error('Transcription error:', result.error);
            }
        } catch (error) {
            console.error('Transcription error:', error);
            recordingStatus.textContent = '‚ùå Transcription error: ' + error.message;
        }
    }

    async function sendTextForVoiceResponse(text) {
      try {
          recordingStatus.textContent = 'üéµ Generating voice response...';
          
          // S·ª≠ d·ª•ng endpoint /ask ƒë·ªÉ g·ª≠i text v√† nh·∫≠n response
          const response = await fetch('/ask', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ question: text })
          });
          
          const result = await response.json();
          if (result.id && result.text) {
              recordingStatus.textContent = 'üéµ Voice response ready!';
              console.log('Voice response ID:', result.id, 'Text:', result.text);
              
              // Hi·ªÉn th·ªã queue info
              queueInfo.style.display = 'block';
              
              // B·∫Øt ƒë·∫ßu polling tr·∫°ng th√°i ƒë·ªÉ l·∫•y v√† ph√°t audio
              pollAndPlayAudio(result.id);
          } else {
              recordingStatus.textContent = '‚ùå No response received';
          }
      } catch (error) {
          console.error('Voice response error:', error);
          recordingStatus.textContent = '‚ùå Voice response failed: ' + error.message;
      }
    }

    // H√†m t·ª± ƒë·ªông polling tr·∫°ng th√°i v√† ph√°t audio khi c√≥
    let audioQueue = {
        items: [],
        isPlaying: false,
        currentTaskId: null
    };

    // H√†m c·∫£i ti·∫øn pollAndPlayAudio
    async function pollAndPlayAudio(taskId) {
        audioQueue.currentTaskId = taskId;
        let taskCompleted = false;
        let quickAudioAdded = false;
        
        // Hi·ªÉn th·ªã queue status
        updateQueueStatus('Waiting for audio...');
        
        // Polling cho ƒë·∫øn khi task ho√†n th√†nh
        while (!taskCompleted) {
            try {
                const resp = await fetch(`/status/${taskId}`);
                const status = await resp.json();
                
                // Th√™m quick audio n·∫øu c√≥ (∆∞u ti√™n cao)
                if (status.quick_audio_url && !quickAudioAdded) {
                    addToAudioQueue({
                        url: status.quick_audio_url,
                        type: 'quick',
                        priority: 10, // Highest priority
                        taskId: taskId
                    });
                    quickAudioAdded = true;
                    updateQueueStatus('Quick audio queued');
                }
                
                // Th√™m full audio parts
                if (Array.isArray(status.full_audio_urls)) {
                    status.full_audio_urls.forEach((url, index) => {
                        if (!isAudioInQueue(url)) {
                            addToAudioQueue({
                                url: url,
                                type: 'full',
                                priority: 5 - index, // Earlier parts get higher priority
                                taskId: taskId
                            });
                        }
                    });
                }
                
                // Ki·ªÉm tra n·∫øu c√≥ merged audio (s·∫Ω thay th·∫ø c√°c parts)
                if (status.full_merged_url) {
                    // Clear queue for this task and add merged audio
                    clearQueueForTask(taskId);
                    addToAudioQueue({
                        url: status.full_merged_url,
                        type: 'merged',
                        priority: 8, // Higher than full parts
                        taskId: taskId
                    });
                }
                
                // T·ª± ƒë·ªông b·∫Øt ƒë·∫ßu ph√°t n·∫øu ch∆∞a ph√°t
                if (!audioQueue.isPlaying && audioQueue.items.length > 0) {
                    playNextInQueue();
                }
                
                // Ki·ªÉm tra n·∫øu task ƒë√£ ho√†n th√†nh
                if (status.status === 'full_ready' || status.full_merged_url) {
                    // ƒê·ª£i queue c·ªßa task n√†y ph√°t xong
                    const hasUnplayed = audioQueue.items.some(item => 
                        item.taskId === taskId && !item.played
                    );
                    
                    if (!hasUnplayed) {
                        taskCompleted = true;
                        recordingStatus.textContent = '‚úÖ Finished!';
                        updateQueueStatus('Completed');
                        
                        // Clean up old audio files
                        await cleanupOldAudio();
                        break;
                    }
                }
                
                // Hi·ªÉn th·ªã th√¥ng tin queue
                updateQueueDisplay();
                
            } catch (e) {
                console.error('Polling error:', e);
                updateQueueStatus('Polling error, retrying...');
                // N·∫øu l·ªói nhi·ªÅu l·∫ßn, th·ª≠ l·∫°i sau
                await new Promise(r => setTimeout(r, 2000));
            }
            
            // Poll interval
            await new Promise(r => setTimeout(r, 1000));
        }
    }

    function addToAudioQueue(item) {
        // Ki·ªÉm tra tr√πng l·∫∑p
        if (isAudioInQueue(item.url)) return;
        
        item.addedAt = Date.now();
        item.played = false;
        
        audioQueue.items.push(item);
        
        // S·∫Øp x·∫øp: priority cao tr∆∞·ªõc, r·ªìi ƒë·∫øn th·ªùi gian th√™m
        audioQueue.items.sort((a, b) => {
            if (b.priority !== a.priority) {
                return b.priority - a.priority;
            }
            return a.addedAt - b.addedAt;
        });
        
        console.log(`Audio queued: ${item.url} (priority: ${item.priority})`);
    }

    function isAudioInQueue(url) {
        return audioQueue.items.some(item => item.url === url);
    }

    function clearQueueForTask(taskId) {
        audioQueue.items = audioQueue.items.filter(item => item.taskId !== taskId);
    }

    async function playNextInQueue() {
        if (audioQueue.isPlaying || audioQueue.items.length === 0) return;
        
        // T√¨m item ch∆∞a ph√°t ƒë·∫ßu ti√™n
        const nextItem = audioQueue.items.find(item => !item.played);
        if (!nextItem) return;
        
        audioQueue.isPlaying = true;
        nextItem.played = true;
        nextItem.startedAt = Date.now();
        
        try {
            recordingStatus.textContent = `üîä Playing ${nextItem.type} response...`;
            updateQueueStatus(`Playing ${nextItem.type} audio...`);
            
            // Ph√°t audio
            await playAudioUrl(nextItem.url);
            
            nextItem.endedAt = Date.now();
            nextItem.duration = nextItem.endedAt - nextItem.startedAt;
            
            console.log(`Played: ${nextItem.url} (${Math.round(nextItem.duration/1000)}s)`);
            
        } catch (error) {
            console.error('Playback failed:', error);
            // Mark as played ƒë·ªÉ kh√¥ng b·ªã k·∫πt
            nextItem.endedAt = Date.now();
            nextItem.error = true;
            updateQueueStatus('Playback error');
        }
        
        audioQueue.isPlaying = false;
        
        // T·ª± ƒë·ªông ph√°t ti·∫øp n·∫øu c√≤n
        if (audioQueue.items.some(item => !item.played)) {
            setTimeout(playNextInQueue, 100); // Small delay
        } else {
            updateQueueStatus('Queue empty');
        }
        
        updateQueueDisplay();
    }

    function updateQueueDisplay() {
        const waiting = audioQueue.items.filter(item => !item.played).length;
        const playing = audioQueue.isPlaying ? 1 : 0;
        const total = audioQueue.items.length;
        
        queueStatus.innerHTML = `
            <div>Playing: ${playing}</div>
            <div>Waiting: ${waiting}</div>
            <div>Total: ${total}</div>
        `;
        
        console.log(`Queue: ${playing} playing, ${waiting} waiting, ${total} total`);
    }

    function updateQueueStatus(message) {
        queueStatus.innerHTML = `<div>${message}</div>`;
    }

    // Cleanup old audio files
    async function cleanupOldAudio() {
        try {
            // L·∫•y danh s√°ch c√°c file c≈© ƒë·ªÉ x√≥a
            const response = await fetch('/cleanup_old_audio', {
                method: 'POST',
                headers: {'Content-Type': 'application/json'},
                body: JSON.stringify({ olderThan: 300 }) // 5 ph√∫t
            });
            const result = await response.json();
            console.log('Cleanup result:', result);
        } catch (e) {
            console.error('Cleanup failed:', e);
        }
    }

    // H√†m ph√°t audio t·ª´ url, tr·∫£ promise khi ph√°t xong
    function playAudioUrl(url) {
        return new Promise((resolve) => {
            recordedAudio.src = url;
            recordedAudio.play().then(() => {
                console.log('Audio playback started:', url);
            }).catch(error => {
                console.error('Audio playback failed:', error);
                resolve(); // V·∫´n resolve ƒë·ªÉ ti·∫øp t·ª•c queue
            });
            
            recordedAudio.onended = () => {
                console.log('Audio playback ended:', url);
                resolve();
            };
            
            recordedAudio.onerror = () => {
                console.error('Audio playback error:', url);
                resolve();
            };
            
            // Timeout ƒë·ªÉ tr√°nh treo n·∫øu audio kh√¥ng play
            setTimeout(() => {
                resolve();
            }, 30000); // 30 seconds timeout
        });
    }

    // Event listeners
    recordBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', stopRecording);

    // Event cho input file
    const wavInput = document.getElementById('wavInput');
    const wavDropZone = document.getElementById('wavDropZone');
    const wavStatus = document.getElementById('wavStatus');
    
    wavInput.addEventListener('change', (e) => {
        if (e.target.files && e.target.files[0]) {
            handleWavFile(e.target.files[0]);
        }
    });
    
    // Drag & drop
    wavDropZone.addEventListener('dragover', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#e0ffe0';
    });
    
    wavDropZone.addEventListener('dragleave', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
    });
    
    wavDropZone.addEventListener('drop', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
        if (e.dataTransfer.files && e.dataTransfer.files[0]) {
            handleWavFile(e.dataTransfer.files[0]);
        }
    });
    
    wavDropZone.addEventListener('click', () => {
        wavInput.click();
    });

    // Auto-stop recording after 30 seconds
    setInterval(() => {
        if (isRecording) {
            const recordingTime = audioChunks.length; // approx seconds
            if (recordingTime >= 30) {
                stopRecording();
                recordingStatus.textContent = '‚è∞ Auto-stopped after 30 seconds';
            }
        }
    }, 1000);

    // Log browser support
    console.log('Browser audio support check:');
    console.log('- MediaRecorder supported:', typeof MediaRecorder !== 'undefined');
    console.log('- AudioContext supported:', typeof (window.AudioContext || window.webkitAudioContext) !== 'undefined');
    console.log('- OfflineAudioContext supported:', typeof (window.OfflineAudioContext || window.webkitOfflineAudioContext) !== 'undefined');
</script>
  </body>
</html>