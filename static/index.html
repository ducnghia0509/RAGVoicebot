<!doctype html>
<html lang="vi">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Simple RAG Voicebot UI - Fixed</title>
    <style>
      body{font-family: Arial, Helvetica, sans-serif; padding:20px}
      textarea{width:100%;height:120px}
      button{padding:10px 16px;margin-top:8px}
      #log{background:#111;color:#bfbfbf;padding:10px;height:200px;overflow:auto;font-size:12px}
    </style>
  </head>
  <body>
    <h2>Simple RAG Voicebot (Voice2Voice)</h2>
    <style>
      /* ·∫®n c√°c ph·∫ßn nh·∫≠p/g√µ text, log, player */
      #question, #askBtn, #respText, #player, #log { display: none !important; }
    </style>

    <!-- Th√™m v√†o body c·ªßa index.html -->
<h3>Voice Input</h3>
<div>
    <button id="recordBtn" style="padding: 12px 24px; font-size: 16px; background: #4CAF50; color: white; border: none; border-radius: 4px;">
        üé§ Start Recording
    </button>
    <button id="stopBtn" style="padding: 12px 24px; font-size: 16px; background: #f44336; color: white; border: none; border-radius: 4px; display: none;">
        ‚èπÔ∏è Stop Recording
    </button>
    <span id="recordingStatus" style="margin-left: 10px; color: #666;"></span>
</div>

<div id="audioVisualizer" style="margin: 10px 0; height: 60px; border: 1px solid #ddd;"></div>

<audio id="recordedAudio" controls style="width: 100%; margin: 10px 0;"></audio>

<script>
    // Voice recording functionality
    let mediaRecorder = null;
    let audioChunks = [];
    let isRecording = false;
    let audioContext = null;
    let analyser = null;
    let dataArray = null;
    let canvas = null;
    let canvasCtx = null;

    const recordBtn = document.getElementById('recordBtn');
    const stopBtn = document.getElementById('stopBtn');
    const recordingStatus = document.getElementById('recordingStatus');
    const audioVisualizer = document.getElementById('audioVisualizer');
    const recordedAudio = document.getElementById('recordedAudio');
    // const questionTextarea = document.getElementById('question');

    // Setup audio visualizer
    function setupVisualizer() {
        canvas = document.createElement('canvas');
        canvas.width = audioVisualizer.clientWidth;
        canvas.height = audioVisualizer.clientHeight;
        audioVisualizer.appendChild(canvas);
        canvasCtx = canvas.getContext('2d');
    }

    // Start recording
    async function startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // Setup audio context for visualization
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);
            
            // Setup visualizer
            if (!canvas) setupVisualizer();
            
            // Start visualization
            drawVisualizer();
            
            // Setup media recorder
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunks.push(event.data);
                }
            };
            
            mediaRecorder.onstop = async () => {
                // Create audio blob
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(audioBlob);
                recordedAudio.src = audioUrl;
                
                // Transcribe audio
                await transcribeAudio(audioBlob);
                
                // Cleanup
                stream.getTracks().forEach(track => track.stop());
                if (audioContext) audioContext.close();
            };
            
            // Start recording
            mediaRecorder.start(1000); // Collect data every second
            isRecording = true;
            
            recordBtn.style.display = 'none';
            stopBtn.style.display = 'inline-block';
            recordingStatus.textContent = 'üé§ Recording...';
            recordingStatus.style.color = '#f44336';
            
        } catch (error) {
            console.error('Error starting recording:', error);
            alert('Cannot access microphone. Please check permissions.');
        }
    }

    // Stop recording
    function stopRecording() {
        if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚úÖ Recording complete';
            recordingStatus.style.color = '#4CAF50';
        }
    }

    // Draw audio visualizer
    function drawVisualizer() {
        if (!isRecording || !analyser || !canvasCtx) return;
        
        requestAnimationFrame(drawVisualizer);
        
        analyser.getByteFrequencyData(dataArray);
        
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = (canvas.width / dataArray.length) * 2.5;
        let barHeight;
        let x = 0;
        
        for (let i = 0; i < dataArray.length; i++) {
            barHeight = dataArray[i] / 2;
            
            canvasCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
            canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
            
            x += barWidth + 1;
        }
    }
    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
      });
    }


    // Transcribe audio to text
    async function transcribeAudio(audioBlob) {
        try {
            recordingStatus.textContent = '‚è≥ Transcribing...';
            
            // Convert to base64
            const base64Audio = await blobToBase64(audioBlob);
            
            // Use new endpoint
            const response = await fetch('/transcribe_base64', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    audio_data: base64Audio.split(',')[1],
                    sample_rate: 16000,
                    stream: false,
                    mime_type: audioBlob.type
                })
            });
            
            const result = await response.json();
            if (result.success && result.text) {
                recordingStatus.textContent = `‚úÖ Transcribed: ${result.text.substring(0, 50)}...`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                recordingStatus.textContent = '‚ùå Transcription failed';
                console.error('Transcription error:', result.error);
            }
        } catch (error) {
            console.error('Transcription error:', error);
            recordingStatus.textContent = '‚ùå Transcription error';
        }
    }

    async function sendTextForVoiceResponse(text) {
      try {
          recordingStatus.textContent = 'üéµ Generating voice response...';
          
          const response = await fetch('/ask', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ question: text })
          });
          
          const result = await response.json();
          if (result.id && result.text) {
              recordingStatus.textContent = 'üéµ Voice response ready!';
              // B·∫°n c√≥ th·ªÉ t·ª± ƒë·ªông play audio response ·ªü ƒë√¢y
              console.log('Voice response ID:', result.id, 'Text:', result.text);
          }
      } catch (error) {
          console.error('Voice response error:', error);
          recordingStatus.textContent = '‚ùå Voice response failed';
      }
    }


    // Event listeners
    recordBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', stopRecording);

    // Auto-stop recording after 30 seconds
    setInterval(() => {
        if (isRecording) {
            const recordingTime = audioChunks.length; // approx seconds
            if (recordingTime >= 30) {
                stopRecording();
            }
        }
    }, 1000);
</script>
  </body>
</html>