<!doctype html>
<html lang="vi">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Simple RAG Voicebot UI - Fixed</title>
    <style>
      body{font-family: Arial, Helvetica, sans-serif; padding:20px}
      textarea{width:100%;height:120px}
      button{padding:10px 16px;margin-top:8px}
      #log{background:#111;color:#bfbfbf;padding:10px;height:200px;overflow:auto;font-size:12px}
    </style>
  </head>
  <body>
    <style>
      /* ·∫®n c√°c ph·∫ßn nh·∫≠p/g√µ text, log, player */
      #question, #askBtn, #respText, #player, #log { display: none !important; }
    </style>

    <!-- Th√™m v√†o body c·ªßa index.html -->
<h3>Voice Input</h3>
<div>
    <button id="recordBtn" style="padding: 12px 24px; font-size: 16px; background: #4CAF50; color: white; border: none; border-radius: 4px;">
        üé§ Start Recording
    </button>
    <button id="stopBtn" style="padding: 12px 24px; font-size: 16px; background: #f44336; color: white; border: none; border-radius: 4px; display: none;">
        ‚èπÔ∏è Stop Recording
    </button>
    <span id="recordingStatus" style="margin-left: 10px; color: #666;"></span>
</div>

<div id="audioVisualizer" style="margin: 10px 0; height: 60px; border: 1px solid #ddd;"></div>

<audio id="recordedAudio" controls style="width: 100%; margin: 10px 0;"></audio>

    <!-- Box th·∫£ file WAV -->
    <div style="margin: 20px 0; padding: 16px; border: 2px dashed #aaa; border-radius: 8px; background: #fafafa; text-align: center;">
        <strong>Ho·∫∑c th·∫£ file WAV v√†o ƒë√¢y ƒë·ªÉ test:</strong><br>
        <input type="file" id="wavInput" accept="audio/wav,audio/x-wav" style="margin-top: 10px;">
        <div id="wavDropZone" style="margin-top: 10px; padding: 20px; border: 2px dashed #4CAF50; border-radius: 6px; background: #f6fff6; color: #333; cursor: pointer;">K√©o v√† th·∫£ file WAV v√†o ƒë√¢y</div>
        <div id="wavStatus" style="margin-top: 8px; color: #666; font-size: 14px;"></div>
    </div>

<script>
    // Voice recording functionality
    let mediaRecorder = null;
    let audioChunks = [];
    let isRecording = false;
    let audioContext = null;
    let analyser = null;
    let dataArray = null;
    let canvas = null;
    let canvasCtx = null;

    const recordBtn = document.getElementById('recordBtn');
    const stopBtn = document.getElementById('stopBtn');
    const recordingStatus = document.getElementById('recordingStatus');
    const audioVisualizer = document.getElementById('audioVisualizer');
    const recordedAudio = document.getElementById('recordedAudio');
    // const questionTextarea = document.getElementById('question');

    // Setup audio visualizer
    function setupVisualizer() {
        canvas = document.createElement('canvas');
        canvas.width = audioVisualizer.clientWidth;
        canvas.height = audioVisualizer.clientHeight;
        audioVisualizer.appendChild(canvas);
        canvasCtx = canvas.getContext('2d');
    }

    // Start recording
    async function startRecording() {
        try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // Setup audio context for visualization
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            analyser.fftSize = 256;
            const bufferLength = analyser.frequencyBinCount;
            dataArray = new Uint8Array(bufferLength);
            
            // Setup visualizer
            if (!canvas) setupVisualizer();
            
            // Start visualization
            drawVisualizer();
            
            // Setup media recorder
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    audioChunks.push(event.data);
                }
            };
            
            mediaRecorder.onstop = async () => {
                // Create audio blob
                const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                const audioUrl = URL.createObjectURL(audioBlob);
                recordedAudio.src = audioUrl;
                
                // Transcribe audio
                await transcribeAudio(audioBlob);
                
                // Cleanup
                stream.getTracks().forEach(track => track.stop());
                if (audioContext) audioContext.close();
            };
            
            // Start recording
            mediaRecorder.start(1000); // Collect data every second
            isRecording = true;
            
            recordBtn.style.display = 'none';
            stopBtn.style.display = 'inline-block';
            recordingStatus.textContent = 'üé§ Recording...';
            recordingStatus.style.color = '#f44336';
            
        } catch (error) {
            console.error('Error starting recording:', error);
            alert('Cannot access microphone. Please check permissions.');
        }
    }

    // Stop recording
    function stopRecording() {
        if (mediaRecorder && isRecording) {
            mediaRecorder.stop();
            isRecording = false;
            
            recordBtn.style.display = 'inline-block';
            stopBtn.style.display = 'none';
            recordingStatus.textContent = '‚úÖ Recording complete';
            recordingStatus.style.color = '#4CAF50';
        }
    }

    // Draw audio visualizer
    function drawVisualizer() {
        if (!isRecording || !analyser || !canvasCtx) return;
        
        requestAnimationFrame(drawVisualizer);
        
        analyser.getByteFrequencyData(dataArray);
        
        canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
        
        const barWidth = (canvas.width / dataArray.length) * 2.5;
        let barHeight;
        let x = 0;
        
        for (let i = 0; i < dataArray.length; i++) {
            barHeight = dataArray[i] / 2;
            
            canvasCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
            canvasCtx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
            
            x += barWidth + 1;
        }
    }
    function blobToBase64(blob) {
      return new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
      });
    }


    // X·ª≠ l√Ω file wav ƒë∆∞·ª£c ch·ªçn ho·∫∑c th·∫£ v√†o
    async function handleWavFile(file) {
        if (!file || !file.type.match(/^audio\/(wav|x-wav)$/)) {
            wavStatus.textContent = '‚ùå File kh√¥ng h·ª£p l·ªá, ch·ªâ nh·∫≠n .wav';
            return;
        }
        wavStatus.textContent = '‚è≥ ƒêang g·ª≠i file ƒë·ªÉ nh·∫≠n ph·∫£n h·ªìi...';
        try {
            // Hi·ªÉn th·ªã file l√™n audio player
            const url = URL.createObjectURL(file);
            recordedAudio.src = url;
            // G·ª≠i file l√™n /transcribe ƒë·ªÉ l·∫•y text
            const formData = new FormData();
            formData.append('file', file);
            formData.append('stream', 'false');
            const resp = await fetch('/transcribe', { method: 'POST', body: formData });
            const result = await resp.json();
            if (result.success && result.text) {
                wavStatus.textContent = `‚úÖ Nh·∫≠n di·ªán: ${result.text.substring(0, 50)}...`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                let msg = '‚ùå Nh·∫≠n di·ªán th·∫•t b·∫°i';
                if (!result.text || (typeof result.text === 'string' && result.text.trim() === '')) {
                    msg = 'Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i n√†o t·ª´ file WAV.';
                }
                wavStatus.textContent = msg;
            }
        } catch (e) {
            wavStatus.textContent = '‚ùå L·ªói khi g·ª≠i file WAV';
            console.error(e);
        }
    }


    // Transcribe audio to text
    async function transcribeAudio(audioBlob) {
        try {
            recordingStatus.textContent = '‚è≥ Transcribing...';
            
            // Convert to base64
            const base64Audio = await blobToBase64(audioBlob);
            
            // Use new endpoint
            const response = await fetch('/transcribe_base64', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    audio_data: base64Audio.split(',')[1],
                    sample_rate: 16000,
                    stream: false,
                    mime_type: audioBlob.type
                })
            });
            
            const result = await response.json();
            if (result.success && result.text) {
                recordingStatus.textContent = `‚úÖ Transcribed: ${result.text.substring(0, 50)}...`;
                // G·ª≠i ti·∫øp text n√†y ƒë·ªÉ nh·∫≠n audio ph·∫£n h·ªìi
                await sendTextForVoiceResponse(result.text);
            } else {
                let msg = '‚ùå Transcription failed';
                if (!result.text || (typeof result.text === 'string' && result.text.trim() === '')) {
                    msg = 'Kh√¥ng nh·∫≠n ƒë∆∞·ª£c l·ªùi n√≥i n√†o t·ª´ ng∆∞·ªùi d√πng, c√≥ l·∫Ω h·ªç ch∆∞a n√≥i g√¨, ho·∫∑c mic c√≥ v·∫•n ƒë·ªÅ';
                }
                recordingStatus.textContent = msg;
                console.error('Transcription error:', result.error);
            }
        } catch (error) {
            console.error('Transcription error:', error);
            recordingStatus.textContent = '‚ùå Transcription error';
        }
    }

    async function sendTextForVoiceResponse(text) {
      try {
          recordingStatus.textContent = 'üéµ Generating voice response...';
          
          const response = await fetch('/ask', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ question: text })
          });
          
          const result = await response.json();
          if (result.id && result.text) {
              recordingStatus.textContent = 'üéµ Voice response ready!';
              // B·∫°n c√≥ th·ªÉ t·ª± ƒë·ªông play audio response ·ªü ƒë√¢y
              console.log('Voice response ID:', result.id, 'Text:', result.text);
                  // B·∫Øt ƒë·∫ßu polling tr·∫°ng th√°i ƒë·ªÉ l·∫•y v√† ph√°t audio
                  pollAndPlayAudio(result.id);
          }
      } catch (error) {
          console.error('Voice response error:', error);
          recordingStatus.textContent = '‚ùå Voice response failed';
      }
    }


        // H√†m t·ª± ƒë·ªông polling tr·∫°ng th√°i v√† ph√°t audio khi c√≥
        let lastPlayedQuick = null;
        let lastPlayedFull = [];


        // Bi·∫øn global ƒë·ªÉ qu·∫£n l√Ω
        let audioQueue = {
            items: [],
            isPlaying: false,
            currentTaskId: null
        };

        // H√†m c·∫£i ti·∫øn pollAndPlayAudio
        async function pollAndPlayAudio(taskId) {
            audioQueue.currentTaskId = taskId;
            let taskCompleted = false;
            let quickAudioAdded = false;
            
            // Polling cho ƒë·∫øn khi task ho√†n th√†nh
            while (!taskCompleted) {
                try {
                    const resp = await fetch(`/status/${taskId}`);
                    const status = await resp.json();
                    
                    // Th√™m quick audio n·∫øu c√≥ (∆∞u ti√™n cao)
                    if (status.quick_audio_url && !quickAudioAdded) {
                        addToAudioQueue({
                            url: status.quick_audio_url,
                            type: 'quick',
                            priority: 10, // Highest priority
                            taskId: taskId
                        });
                        quickAudioAdded = true;
                    }
                    
                    // Th√™m full audio parts
                    if (Array.isArray(status.full_audio_urls)) {
                        status.full_audio_urls.forEach((url, index) => {
                            if (!isAudioInQueue(url)) {
                                addToAudioQueue({
                                    url: url,
                                    type: 'full',
                                    priority: 5 - index, // Earlier parts get higher priority
                                    taskId: taskId
                                });
                            }
                        });
                    }
                    
                    // Ki·ªÉm tra n·∫øu c√≥ merged audio (s·∫Ω thay th·∫ø c√°c parts)
                    if (status.full_merged_url) {
                        // Clear queue for this task and add merged audio
                        clearQueueForTask(taskId);
                        addToAudioQueue({
                            url: status.full_merged_url,
                            type: 'merged',
                            priority: 8, // Higher than full parts
                            taskId: taskId
                        });
                    }
                    
                    // T·ª± ƒë·ªông b·∫Øt ƒë·∫ßu ph√°t n·∫øu ch∆∞a ph√°t
                    if (!audioQueue.isPlaying && audioQueue.items.length > 0) {
                        playNextInQueue();
                    }
                    
                    // Ki·ªÉm tra n·∫øu task ƒë√£ ho√†n th√†nh
                    if (status.status === 'full_ready' || status.full_merged_url) {
                        // ƒê·ª£i queue c·ªßa task n√†y ph√°t xong
                        const hasUnplayed = audioQueue.items.some(item => 
                            item.taskId === taskId && !item.played
                        );
                        
                        if (!hasUnplayed) {
                            taskCompleted = true;
                            recordingStatus.textContent = '‚úÖ Finished!';
                            // Clean up old audio files
                            cleanupOldAudio();
                            break;
                        }
                    }
                    
                    // Hi·ªÉn th·ªã th√¥ng tin queue
                    updateQueueDisplay();
                    
                } catch (e) {
                    console.error('Polling error:', e);
                    // N·∫øu l·ªói nhi·ªÅu l·∫ßn, th·ª≠ l·∫°i sau
                    await new Promise(r => setTimeout(r, 2000));
                }
                
                // Poll interval
                await new Promise(r => setTimeout(r, 1000));
            }
        }

        function addToAudioQueue(item) {
            // Ki·ªÉm tra tr√πng l·∫∑p
            if (isAudioInQueue(item.url)) return;
            
            item.addedAt = Date.now();
            item.played = false;
            
            audioQueue.items.push(item);
            
            // S·∫Øp x·∫øp: priority cao tr∆∞·ªõc, r·ªìi ƒë·∫øn th·ªùi gian th√™m
            audioQueue.items.sort((a, b) => {
                if (b.priority !== a.priority) {
                    return b.priority - a.priority;
                }
                return a.addedAt - b.addedAt;
            });
            
            console.log(`Audio queued: ${item.url} (priority: ${item.priority})`);
        }

        function isAudioInQueue(url) {
            return audioQueue.items.some(item => item.url === url);
        }

        function clearQueueForTask(taskId) {
            audioQueue.items = audioQueue.items.filter(item => item.taskId !== taskId);
        }

        async function playNextInQueue() {
            if (audioQueue.isPlaying || audioQueue.items.length === 0) return;
            
            // T√¨m item ch∆∞a ph√°t ƒë·∫ßu ti√™n
            const nextItem = audioQueue.items.find(item => !item.played);
            if (!nextItem) return;
            
            audioQueue.isPlaying = true;
            nextItem.played = true;
            nextItem.startedAt = Date.now();
            
            try {
                recordingStatus.textContent = `üîä Playing ${nextItem.type} response...`;
                
                // Ph√°t audio
                await playAudioUrl(nextItem.url);
                
                nextItem.endedAt = Date.now();
                nextItem.duration = nextItem.endedAt - nextItem.startedAt;
                
                console.log(`Played: ${nextItem.url} (${Math.round(nextItem.duration/1000)}s)`);
                
            } catch (error) {
                console.error('Playback failed:', error);
                // Mark as played ƒë·ªÉ kh√¥ng b·ªã k·∫πt
                nextItem.endedAt = Date.now();
                nextItem.error = true;
            }
            
            audioQueue.isPlaying = false;
            
            // T·ª± ƒë·ªông ph√°t ti·∫øp n·∫øu c√≤n
            if (audioQueue.items.some(item => !item.played)) {
                setTimeout(playNextInQueue, 100); // Small delay
            } else {
                recordingStatus.textContent = 'Queue empty';
            }
            
            updateQueueDisplay();
        }

        function updateQueueDisplay() {
            const waiting = audioQueue.items.filter(item => !item.played).length;
            const playing = audioQueue.isPlaying ? 1 : 0;
            
            // C√≥ th·ªÉ hi·ªÉn th·ªã ·ªü ƒë√¢u ƒë√≥ tr√™n UI
            console.log(`Queue: ${playing} playing, ${waiting} waiting`);
            
            // Ho·∫∑c hi·ªÉn th·ªã trong m·ªôt div
            const queueInfo = document.getElementById('queueInfo');
            if (queueInfo) {
                queueInfo.innerHTML = `
                    <div>Playing: ${playing}</div>
                    <div>Waiting: ${waiting}</div>
                    <div>Total: ${audioQueue.items.length}</div>
                `;
            }
        }

        // Cleanup old audio files
        async function cleanupOldAudio() {
            try {
                // L·∫•y danh s√°ch c√°c file c≈© ƒë·ªÉ x√≥a
                const response = await fetch('/cleanup_old_audio', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ olderThan: 300 }) // 5 ph√∫t
                });
                const result = await response.json();
                console.log('Cleanup result:', result);
            } catch (e) {
                console.error('Cleanup failed:', e);
            }
        }



        // H√†m ph√°t audio t·ª´ url, tr·∫£ promise khi ph√°t xong
        function playAudioUrl(url) {
            return new Promise((resolve) => {
                recordedAudio.src = url;
                recordedAudio.play();
                recordedAudio.onended = () => {
                    resolve();
                };
                recordedAudio.onerror = () => {
                    resolve();
                };
            });
        }


    // Event listeners
    recordBtn.addEventListener('click', startRecording);
    stopBtn.addEventListener('click', stopRecording);

    // Event cho input file
    const wavInput = document.getElementById('wavInput');
    const wavDropZone = document.getElementById('wavDropZone');
    const wavStatus = document.getElementById('wavStatus');
    wavInput.addEventListener('change', (e) => {
        if (e.target.files && e.target.files[0]) {
            handleWavFile(e.target.files[0]);
        }
    });
    // Drag & drop
    wavDropZone.addEventListener('dragover', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#e0ffe0';
    });
    wavDropZone.addEventListener('dragleave', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
    });
    wavDropZone.addEventListener('drop', (e) => {
        e.preventDefault();
        wavDropZone.style.background = '#f6fff6';
        if (e.dataTransfer.files && e.dataTransfer.files[0]) {
            handleWavFile(e.dataTransfer.files[0]);
        }
    });
    wavDropZone.addEventListener('click', () => {
        wavInput.click();
    });

    // Auto-stop recording after 30 seconds
    setInterval(() => {
        if (isRecording) {
            const recordingTime = audioChunks.length; // approx seconds
            if (recordingTime >= 30) {
                stopRecording();
            }
        }
    }, 1000);
</script>
  </body>
</html>