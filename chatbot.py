

import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama
import torch
import os
from typing import List, Dict
import time
import logging
import sys
from io import StringIO

# Custom StreamHandler to force UTF-8 encoding for console output
class UTF8StreamHandler(logging.StreamHandler):
    def __init__(self):
        super().__init__(stream=sys.stdout)
        self.stream = StringIO()
        sys.stdout = self

    def write(self, text):
        if isinstance(text, str):
            sys.__stdout__.write(text.encode('utf-8', errors='replace').decode('utf-8'))
        else:
            sys.__stdout__.write(str(text))

    def flush(self):
        sys.__stdout__.flush()

# Cáº¥u hÃ¬nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        UTF8StreamHandler(),
        logging.FileHandler('processing_log.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Configuration
EMBEDDINGS_JSONL = "embeddings.jsonl"
FAISS_INDEX_PATH = "faiss_index.bin"
# GGUF_MODEL_PATH = r"C:\Users\DELL\Desktop\Test\LocalProjectPackage\Project\VoiceBotBasic\Gemma-2-IT-Q6_K.gguf"  
GGUF_MODEL_PATH = "gemma-3-1b-it-Q8_0.gguf"  
MODEL_NAME = "Alibaba-NLP/gte-multilingual-base"
TOP_K = 5
MAX_TOKENS = 512
TEMPERATURE = 0.7

# Load embedding model
@st.cache_resource
def load_embedding_model():
    start_time = time.time()
    logger.info("Báº¯t Ä‘áº§u táº£i embedding model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = SentenceTransformer(MODEL_NAME, device=device, trust_remote_code=True)
    # model = SentenceTransformer(
    #     r"C:\Users\DELL\.cache\huggingface\hub\models--Alibaba-NLP--gte-multilingual-base",
    #     device=device,
    #     trust_remote_code=True
    # )

    elapsed_time = time.time() - start_time
    logger.info(f"Táº£i embedding model hoÃ n táº¥t. Thá»i gian: {elapsed_time:.2f} giÃ¢y")
    return model

# Load FAISS index and metadata
@st.cache_resource
def load_faiss_and_metadata():
    start_time = time.time()
    logger.info("Báº¯t Ä‘áº§u táº£i FAISS index vÃ  metadata...")
    metadatas = []
    texts = []
    with open(EMBEDDINGS_JSONL, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            data = json.loads(line)
            metadatas.append(data["metadata"])
            texts.append(data["text"])
    index = faiss.read_index(FAISS_INDEX_PATH)
    elapsed_time = time.time() - start_time
    logger.info(f"Táº£i FAISS index vÃ  metadata hoÃ n táº¥t. Thá»i gian: {elapsed_time:.2f} giÃ¢y")
    return index, metadatas, texts

# Retrieve top-k
def retrieve_top_k(query: str, index, embedding_model, metadatas, texts, k: int = TOP_K):
    start_time = time.time()
    logger.info("Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh truy xuáº¥t top-k...")
    
    # BÆ°á»›c 1: Táº¡o embedding cho query
    encode_start = time.time()
    logger.info("BÆ°á»›c 1: Táº¡o embedding cho query...")
    query_emb = embedding_model.encode([query], convert_to_numpy=True).astype(np.float32)
    query_emb = query_emb.reshape(1, -1)
    encode_time = time.time() - encode_start
    logger.info(f"BÆ°á»›c 1 hoÃ n táº¥t. Thá»i gian táº¡o embedding: {encode_time:.2f} giÃ¢y")
    
    # BÆ°á»›c 2: TÃ¬m kiáº¿m FAISS
    search_start = time.time()
    logger.info("BÆ°á»›c 2: TÃ¬m kiáº¿m top-k trong FAISS index...")
    distances, indices = index.search(query_emb, k)
    search_time = time.time() - search_start
    logger.info(f"BÆ°á»›c 2 hoÃ n táº¥t. Thá»i gian tÃ¬m kiáº¿m FAISS: {search_time:.2f} giÃ¢y")
    
    # BÆ°á»›c 3: Xá»­ lÃ½ káº¿t quáº£
    process_start = time.time()
    logger.info("BÆ°á»›c 3: Xá»­ lÃ½ káº¿t quáº£ truy xuáº¥t...")
    results = []
    for i, idx in enumerate(indices[0]):
        if idx >= len(metadatas):
            continue
        metadata = metadatas[idx]
        source_file = metadata.get('source_file', '')
        chunk_index = metadata.get('chunk_index', 0)
        text = texts[idx]
        
        if source_file and os.path.exists(source_file):
            try:
                with open(source_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    chunks = data.get('chunks', []) or data.get('articles', [])
                    if chunk_index < len(chunks):
                        text = chunks[chunk_index].get('text', text)
            except Exception as e:
                logger.warning(f"KhÃ´ng thá»ƒ Ä‘á»c file nguá»“n {source_file}: {e}")
        
        results.append({
            "distance": distances[0][i],
            "metadata": metadata,
            "text": text
        })
    process_time = time.time() - process_start
    logger.info(f"BÆ°á»›c 3 hoÃ n táº¥t. Thá»i gian xá»­ lÃ½ káº¿t quáº£: {process_time:.2f} giÃ¢y")
    
    total_retrieve_time = time.time() - start_time
    logger.info(f"QuÃ¡ trÃ¬nh truy xuáº¥t top-k hoÃ n táº¥t. Tá»•ng thá»i gian: {total_retrieve_time:.2f} giÃ¢y")
    return results

# Load LLM
@st.cache_resource
def load_llm():
    start_time = time.time()
    logger.info("Báº¯t Ä‘áº§u táº£i LLM...")
    llm = Llama(
        model_path=GGUF_MODEL_PATH,
        n_ctx=4096,
        n_threads=8,
        n_gpu_layers=0,
        verbose=False,
        kv_cache_type="standard"
    )
    elapsed_time = time.time() - start_time
    logger.info(f"Táº£i LLM hoÃ n táº¥t. Thá»i gian: {elapsed_time:.2f} giÃ¢y")
    return llm

# Streamlit UI
def main():
    st.title("ðŸ¦™ Chatbot ")
    st.sidebar.info("Cáº¥u hÃ¬nh:\n- Model: Llama GGUF\n- Retrieval: Top-{TOP_K} tá»« FAISS\n- Embedding: gte-multilingual-base")

    start_time = time.time()
    logger.info("Báº¯t Ä‘áº§u khá»Ÿi táº¡o giao diá»‡n vÃ  tÃ i nguyÃªn...")
    
    embedding_model = load_embedding_model()
    index, metadatas, texts = load_faiss_and_metadata()
    llm = load_llm()
    
    resource_load_time = time.time() - start_time
    logger.info(f"Khá»Ÿi táº¡o giao diá»‡n vÃ  tÃ i nguyÃªn hoÃ n táº¥t. Thá»i gian: {resource_load_time:.2f} giÃ¢y")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n..."):
        input_start_time = time.time()
        logger.info("Nháº­n Ä‘Æ°á»£c input tá»« ngÆ°á»i dÃ¹ng")
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Äang tÃ¬m kiáº¿m vÃ  suy nghÄ©..."):
                retrieve_start = time.time()
                logger.info("Báº¯t Ä‘áº§u truy xuáº¥t context...")
                top_chunks = retrieve_top_k(prompt, index, embedding_model, metadatas, texts)
                context = "\n".join([f"- {chunk['text']} (Nguá»“n: {chunk['metadata'].get('source_file', 'Unknown')})" for chunk in top_chunks])
                retrieve_time = time.time() - retrieve_start
                logger.info(f"Truy xuáº¥t context hoÃ n táº¥t. Thá»i gian: {retrieve_time:.2f} giÃ¢y")
                st.info(f"Context retrieved (Top-{TOP_K}):\n{context}")
                
                response_start = time.time()
                logger.info("Báº¯t Ä‘áº§u táº¡o pháº£n há»“i tá»« LLM...")
                response_placeholder = st.empty()
                full_response = ""
                first_token_time = None
                for chunk in llm(
                    f"""
Dá»±a hoÃ n toÃ n vÃ o ná»™i dung trong pháº§n Context, tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng báº±ng tiáº¿ng Viá»‡t sao cho:
- Tráº£ lá»i thÃ¢n thiá»‡n, rÃµ rÃ ng, dá»… hiá»ƒu.
- Äáº§y Ä‘á»§ Ã½ vÃ  chÃ­nh xÃ¡c, Báº®T BUá»˜C CÃ‚U TRáº¢ Lá»œI KHÃ”NG Bá»Š Láº¶P GÃ‚Y KHÃ“ HIá»‚U.
- KhÃ´ng suy diá»…n ngoÃ i Context.
- NÃªu rÃµ cÄƒn cá»© phÃ¡p lÃ½ hoáº·c nguá»“n trÃ­ch (vÃ­ dá»¥: "theo Äiá»u/Luáº­t/Nghá»‹ Ä‘inh,..."). 
Náº¿u khÃ´ng cÃ³, cÃ³ thá»ƒ nÃ³i: "Theo nhÆ° thÃ´ng tin tÃ´i Ä‘Æ°á»£c cung cáº¥p,..."   
- TrÃ¡nh láº·p láº¡i cÃ¢u tráº£ lá»i, khÃ´ng nÃ³i lan man.

Context:
{context}

CÃ¢u há»i: {prompt}

Tráº£ lá»i:
"""
,
                    max_tokens=MAX_TOKENS,
                    temperature=TEMPERATURE,
                    stream=True
                ):
                    if 'choices' in chunk and chunk['choices'][0]['text']:
                        if first_token_time is None:
                            first_token_time = time.time()
                            input_to_first_token_time = first_token_time - input_start_time
                            logger.info(f"Thá»i gian tá»« khi nháº­n input Ä‘áº¿n khi LLM in chá»¯ Ä‘áº§u tiÃªn: {input_to_first_token_time:.2f} giÃ¢y")
                            st.info(f"Thá»i gian tá»« input Ä‘áº¿n chá»¯ Ä‘áº§u tiÃªn: {input_to_first_token_time:.2f} giÃ¢y")
                        full_response += chunk['choices'][0]['text']
                        response_placeholder.markdown(full_response + "â–Œ")
                response_time = time.time() - response_start
                logger.info(f"Táº¡o pháº£n há»“i tá»« LLM hoÃ n táº¥t. Thá»i gian: {response_time:.2f} giÃ¢y")
                
                response_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
                total_processing_time = time.time() - input_start_time
                logger.info(f"Tá»•ng thá»i gian xá»­ lÃ½ cÃ¢u há»i: {total_processing_time:.2f} giÃ¢y")
                st.info(f"Tá»•ng thá»i gian xá»­ lÃ½: {total_processing_time:.2f} giÃ¢y")

if __name__ == "__main__":
    main()